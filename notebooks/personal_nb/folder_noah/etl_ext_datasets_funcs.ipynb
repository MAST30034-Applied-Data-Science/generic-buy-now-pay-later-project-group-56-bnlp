{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType, StructType, IntegerType\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BNPL_Income_ETL\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "INCOME_SDF_PATH = 'income_by_sa2.parquet'\n",
    "POSTCODE_SDF_PATH = 'postcode_data.csv'\n",
    "#DATA_PATH = '../data/tables/external_datasets/'\n",
    "DATA_PATH = '../../../data/tables/external_datasets/'\n",
    "\n",
    "POSTCODES_SUBSET = ['postcode', 'SA2_MAINCODE_2016']\n",
    "\n",
    "POPULATION_PATH = 'population_data.csv'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# workaround to working with ill-formatted ABS XLSX data -> would prefer an api call\n",
    "def load_income_from_csv():\n",
    "\n",
    "    # manually create the schema -> to deal with duplicate column names\n",
    "    schema = StructType() \\\n",
    "        .add(\"SA2\", StringType(), True) \\\n",
    "        .add(\"SA2_NAME\", StringType(), True) \\\n",
    "        .add(\"persons_earners_2014-15\", StringType(), True) \\\n",
    "        .add(\"persons_earners_2015-16\", StringType(), True) \\\n",
    "        .add(\"persons_earners_2016-17\", StringType(), True) \\\n",
    "        .add(\"persons_earners_2017-18\", StringType(), True) \\\n",
    "        .add(\"persons_earners_2018-19\", StringType(), True) \\\n",
    "        .add(\"med_age_earners_2014-15\", StringType(), True) \\\n",
    "        .add(\"med_age_earners_2015-16\", StringType(), True) \\\n",
    "        .add(\"med_age_earners_2016-17\", StringType(), True) \\\n",
    "        .add(\"med_age_earners_2017-18\", StringType(), True) \\\n",
    "        .add(\"med_age_earners_2018-19\", StringType(), True) \\\n",
    "        .add(\"sum_earnings_2014-15\", StringType(), True) \\\n",
    "        .add(\"sum_earnings_2015-16\", StringType(), True) \\\n",
    "        .add(\"sum_earnings_2016-17\", StringType(), True) \\\n",
    "        .add(\"sum_earnings_2017-18\", StringType(), True) \\\n",
    "        .add(\"sum_earnings_2018-19\", StringType(), True) \\\n",
    "        .add(\"median_earnings_2014-15\", StringType(), True) \\\n",
    "        .add(\"median_earnings_2015-16\", StringType(), True) \\\n",
    "        .add(\"median_earnings_2016-17\", StringType(), True) \\\n",
    "        .add(\"median_earnings_2017-18\", StringType(), True) \\\n",
    "        .add(\"median_earnings_2018-19\", StringType(), True) \\\n",
    "        .add(\"mean_earnings_2014-15\", StringType(), True) \\\n",
    "        .add(\"mean_earnings_2015-16\", StringType(), True) \\\n",
    "        .add(\"mean_earnings_2016-17\", StringType(), True) \\\n",
    "        .add(\"mean_earnings_2017-18\", StringType(), True) \\\n",
    "        .add(\"mean_earnings_2018-19\", StringType(), True)\n",
    "\n",
    "    # read in csv conforming to custom schema\n",
    "    income_sdf = spark.read.format(\"csv\") \\\n",
    "                        .option(\"header\", False) \\\n",
    "                        .schema(schema) \\\n",
    "                        .load(DATA_PATH + 'income_data_raw.csv')\n",
    "    # remove header\n",
    "    income_sdf = income_sdf.where(income_sdf['SA2'] != \"SA2\")\n",
    "\n",
    "    return income_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def write_inc_to_pq(income_sdf):\n",
    "    income_sdf.write.mode('overwrite').parquet(DATA_PATH + INCOME_SDF_PATH)\n",
    "    print(\"Wrote to \", DATA_PATH + INCOME_SDF_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def read_postcodes():\n",
    "    postcodes = spark.read.options(header=True) \\\n",
    "                     .csv(DATA_PATH + POSTCODE_SDF_PATH)\n",
    "\n",
    "    # select useful subset for linking\n",
    "    postcodes = postcodes.select(*POSTCODES_SUBSET)\n",
    "\n",
    "    # rename clashing col names\n",
    "    postcodes = postcodes.withColumnRenamed('SA2_MAINCODE_2016', 'sa2_code')\n",
    "\n",
    "    return postcodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# This gives the income data aggregated by postcode\n",
    "def etl_income():\n",
    "\n",
    "    # get sdfs\n",
    "    postcodes = read_postcodes()\n",
    "\n",
    "    # if exists -> read_income() else\n",
    "    income_sdf = load_income_from_csv()\n",
    "\n",
    "    # join income and postcode dataset by SA2 maincode\n",
    "    inc_joined = postcodes \\\n",
    "        .join(income_sdf, postcodes.sa2_code == income_sdf.SA2, \"left\") \\\n",
    "        .na.drop() \\\n",
    "        .distinct()\n",
    "\n",
    "    # n.b postcodes can have multiple SA2 codes & SA2 codes can have multiple postcodes\n",
    "    # aggregate by postcode ->\n",
    "    agg_by_postcode_income = inc_joined \\\n",
    "    .groupBy('postcode') \\\n",
    "    .agg(\n",
    "        F.sum('persons_earners_2018-19').alias('persons_earners_2018-19_sum'),\n",
    "        F.mean('mean_earnings_2018-19').alias('mean_earnings_2018-19_avg'),\n",
    "        F.sum('sum_earnings_2018-19').alias('sum_earnings_2018-19_sum'),\n",
    "        F.mean('median_earnings_2018-19').alias('median_earnings_2018-19_avg'),\n",
    "        F.mean('med_age_earners_2018-19').alias('med_age_earners_2018-19_avg')\n",
    "    ) \\\n",
    "    .orderBy('postcode')\n",
    "\n",
    "    return agg_by_postcode_income"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "+--------+---------------------------+-------------------------+------------------------+---------------------------+---------------------------+\n|postcode|persons_earners_2018-19_sum|mean_earnings_2018-19_avg|sum_earnings_2018-19_sum|median_earnings_2018-19_avg|med_age_earners_2018-19_avg|\n+--------+---------------------------+-------------------------+------------------------+---------------------------+---------------------------+\n|    0200|                      552.0|                  19479.0|             1.0752338E7|                    10433.0|                       23.0|\n|    0800|                     5632.0|                  74682.0|            4.20609031E8|                    57789.0|                       33.0|\n|    0801|                     5632.0|                  74682.0|            4.20609031E8|                    57789.0|                       33.0|\n|    0804|                     1810.0|                  88303.0|            1.59828824E8|                    71724.0|                       40.0|\n|    0810|                    21932.0|        69868.83333333333|           1.574969237E9|         60190.166666666664|         39.583333333333336|\n+--------+---------------------------+-------------------------+------------------------+---------------------------+---------------------------+",
      "text/html": "<table border='1'>\n<tr><th>postcode</th><th>persons_earners_2018-19_sum</th><th>mean_earnings_2018-19_avg</th><th>sum_earnings_2018-19_sum</th><th>median_earnings_2018-19_avg</th><th>med_age_earners_2018-19_avg</th></tr>\n<tr><td>0200</td><td>552.0</td><td>19479.0</td><td>1.0752338E7</td><td>10433.0</td><td>23.0</td></tr>\n<tr><td>0800</td><td>5632.0</td><td>74682.0</td><td>4.20609031E8</td><td>57789.0</td><td>33.0</td></tr>\n<tr><td>0801</td><td>5632.0</td><td>74682.0</td><td>4.20609031E8</td><td>57789.0</td><td>33.0</td></tr>\n<tr><td>0804</td><td>1810.0</td><td>88303.0</td><td>1.59828824E8</td><td>71724.0</td><td>40.0</td></tr>\n<tr><td>0810</td><td>21932.0</td><td>69868.83333333333</td><td>1.574969237E9</td><td>60190.166666666664</td><td>39.583333333333336</td></tr>\n</table>\n"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_df = etl_income()\n",
    "income_df.limit(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def population_preprocess(data): # ANUJ BUNGLA\n",
    "\n",
    "    cols_to_keep = ['sa2_maincode_2016', 'sa2_name_2016', 'erp_2021']\n",
    "    population_df = data.select(*cols_to_keep)\n",
    "\n",
    "    population_df = population_df \\\n",
    "                    .withColumn(\"erp_2021\", F.col('erp_2021').cast(IntegerType()))\n",
    "\n",
    "    population_df = population_df \\\n",
    "                    .withColumnRenamed('sa2_name_2016', 'suburb') \\\n",
    "                    .withColumnRenamed('erp_2021', 'estimated_region_population_2021')\n",
    "\n",
    "    population_df = population_df.filter(F.col('estimated_region_population_2021') > 0)\n",
    "\n",
    "    return population_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def read_population():\n",
    "    population = spark.read.option(\"header\", True).csv(DATA_PATH + POPULATION_PATH)\n",
    "    return population"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def etl_population():\n",
    "    # read in the population/postcode data from file\n",
    "    pop_sdf = read_population()\n",
    "    postcodes = read_postcodes()\n",
    "\n",
    "    # use ANUJ's script to preprocess the df\n",
    "    pop_sdf = population_preprocess(pop_sdf)\n",
    "\n",
    "    # join the population data with postcodes\n",
    "    population_joined = pop_sdf.join(postcodes, pop_sdf['sa2_maincode_2016'] == postcodes['sa2_code'], \"left\").na.drop().distinct()\n",
    "\n",
    "    # aggregate population data by postcode\n",
    "    agg_by_postcode_population = population_joined \\\n",
    "        .groupBy('postcode') \\\n",
    "        .agg(\n",
    "        F.sum('estimated_region_population_2021').alias('estimated_region_population_2021_sum'),\n",
    "    ) \\\n",
    "        .orderBy('postcode')\n",
    "\n",
    "    return agg_by_postcode_population"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# testing\n",
    "pop_data = etl_population()\n",
    "pop_data.limit(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# one function -> get income dataset\n",
    "# another function -> join_with_master\n",
    "\n",
    "# joins the income ds with consumer transactions data\n",
    "def join_ext_with_master(income_sdf, pop_sdf, transactions):\n",
    "\n",
    "    # rename postcode cols before joining\n",
    "    pop_sdf = pop_sdf.withColumnRenamed('postcode', 'postcode_pset')\n",
    "    income_sdf = pop_sdf.withColumnRenamed('postcode', 'postcode_iset')\n",
    "\n",
    "    transactions_ = transactions\\\n",
    "        .join(pop_sdf, transactions['postcode'] == pop_sdf['postcode_pset'], how='left')\\\n",
    "        .drop('postcode_pset')\\\n",
    "        .join(income_sdf, transactions['postcode'] == income_sdf['postcode_iset'], how='left')\\\n",
    "        .drop('postcode_iset')\n",
    "    return transactions_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}