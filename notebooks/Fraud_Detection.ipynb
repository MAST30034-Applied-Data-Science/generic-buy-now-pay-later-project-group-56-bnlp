{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Model\n",
    "\n",
    "K-means clustering will be used to predict which transactions are fraudulent. Fraudulent transactions will most likely have similar features/behavior with one another, therefore they will all be in the same cluster. We can then treat the cluster with the highest fraud probability as the cluster with all fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing for K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import mean, countDistinct\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Importing model libraries\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, VectorAssembler)\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 17:59:41 WARN Utils: Your hostname, LAPTOP-03OFAS5P resolves to a loopback address: 127.0.1.1; using 172.20.154.107 instead (on interface eth0)\n",
      "22/10/05 17:59:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 17:59:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BNPL Project\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "sdf = spark.read.parquet(\"../data/curated/process_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only consider instances with non-null fraud probabilities. It is assumed that users and merchants with no fraud probabilities are deemed to be not suspicious, therefore the probability of their transactions being fraudulent will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate instances with null fraud probabilities\n",
    "sdf_fraudless = sdf.filter(F.col(\"merchant_fraud_probability\").isNull() | F.col(\"user_fraud_probability\").isNull())\n",
    "sdf_fraud = sdf.filter(F.col(\"merchant_fraud_probability\").isNotNull() & F.col(\"user_fraud_probability\").isNotNull())\n",
    "sdf = sdf_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting attributes\n",
    "sdf = sdf.withColumn('postcode', sdf[\"postcode\"].cast(IntegerType()))\n",
    "features = ['merchant_abn', 'consumer_id', 'dollar_value', 'postcode', 'gender', 'revenue', 'rate', 'category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical features to integer and then one hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Discretisation\n",
    "stringToNum = StringIndexer(inputCol= 'gender', outputCol= 'genderNum')\n",
    "output_data = stringToNum.fit(sdf).transform(sdf)\n",
    "\n",
    "stringToNum = StringIndexer(inputCol= 'revenue', outputCol= 'revenueNum')\n",
    "output_data = stringToNum.fit(output_data).transform(output_data)\n",
    "\n",
    "stringToNum = StringIndexer(inputCol= 'category', outputCol= 'categoryNum')\n",
    "output_data = stringToNum.fit(output_data).transform(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "encoder = OneHotEncoder(inputCol= 'genderNum', outputCol = 'genderVec')\n",
    "onehotdata = encoder.fit(output_data).transform(output_data)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol= 'revenueNum', outputCol = 'revenueVec')\n",
    "onehotdata = encoder.fit(onehotdata).transform(onehotdata)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol= 'categoryNum', outputCol = 'categoryVec')\n",
    "onehotdata = encoder.fit(onehotdata).transform(onehotdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert features to a single vector and standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to vector\n",
    "assembler1 = VectorAssembler(\n",
    "inputCols= ['merchant_abn', 'consumer_id', 'dollar_value', 'postcode', 'genderVec', 'revenueVec', 'rate', 'categoryVec'],\n",
    "outputCol='features')\n",
    "result = assembler1.transform(onehotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# standardizing the feature vector\n",
    "scale = StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale = scale.fit(result)\n",
    "data_scale_output = data_scale.transform(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35692111925566483"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build K-means clustering model\n",
    "import numpy as np\n",
    "cost = np.zeros(11)\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "KMeans_algo=KMeans(featuresCol='standardized', k= 2)\n",
    "KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
    "output=KMeans_fit.transform(data_scale_output)  \n",
    "score=evaluator.evaluate(output)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>prediction</th><th>avg(merchant_fraud_probability)</th></tr>\n",
       "<tr><td>1</td><td>0.30587708283902687</td></tr>\n",
       "<tr><td>0</td><td>0.29367707319589614</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------------------------+\n",
       "|prediction|avg(merchant_fraud_probability)|\n",
       "+----------+-------------------------------+\n",
       "|         1|            0.30587708283902687|\n",
       "|         0|            0.29367707319589614|\n",
       "+----------+-------------------------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean merchant fraud probability for each cluster\n",
    "output.groupBy(\"prediction\").mean(\"merchant_fraud_probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>prediction</th><th>avg(user_fraud_probability)</th></tr>\n",
       "<tr><td>1</td><td>0.15422160496640785</td></tr>\n",
       "<tr><td>0</td><td>0.15355481750583094</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------------------------+\n",
       "|prediction|avg(user_fraud_probability)|\n",
       "+----------+---------------------------+\n",
       "|         1|        0.15422160496640785|\n",
       "|         0|        0.15355481750583094|\n",
       "+----------+---------------------------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean user fraud probability for each cluster\n",
    "output.groupBy(\"prediction\").mean(\"user_fraud_probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cluster 0 has higher mean merchant and user fraud probabilities, we will treat all the transactions in this cluster as fraud. These transactions will not be included in the ranking system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Fraud Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149013"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of fraud transactions\n",
    "output.filter(F.col(\"prediction\") == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 206014 transactions will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT order_id)|\n",
      "+------------------------+\n",
      "|                  472823|\n",
      "+------------------------+\n",
      "\n",
      "472823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ensure that each transaction has a unique order_id which will be used for joining\n",
    "print(sdf.select(countDistinct(\"order_id\")))\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "323810"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only join transactions from cluster 1\n",
    "sdf = sdf.join(output.filter(F.col(\"prediction\") == 1), on=[\"order_id\"], how=\"leftSemi\")\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13465143"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two dataframes\n",
    "sdf = sdf.union(sdf_fraudless)\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save data for ranking\n",
    "sdf.write.mode('overwrite').parquet('../data/curated/fraudless_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
