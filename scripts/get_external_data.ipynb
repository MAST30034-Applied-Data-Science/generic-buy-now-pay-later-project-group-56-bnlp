{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import folium\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from urllib.request import urlretrieve\n",
    "from owslib.wfs import WebFeatureService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 02:03:45 WARN Utils: Your hostname, DESKTOP-1ML24G5 resolves to a loopback address: 127.0.1.1; using 172.30.25.153 instead (on interface eth0)\n",
      "22/09/19 02:03:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 02:03:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BNPL Get external data\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of root directory.\n",
    "\n",
    "root_dir = '../data/tables/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../data/tables/test_external_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/anujb/OneDrive/Desktop/gitHubStuff/generic-buy-now-pay-later-project-group-56-bnlp/scripts/get_external_data.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/anujb/OneDrive/Desktop/gitHubStuff/generic-buy-now-pay-later-project-group-56-bnlp/scripts/get_external_data.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create \"external_datasets\" folder under the root directory where all the external data will be stored.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/anujb/OneDrive/Desktop/gitHubStuff/generic-buy-now-pay-later-project-group-56-bnlp/scripts/get_external_data.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m external_data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest_external_datasets\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/anujb/OneDrive/Desktop/gitHubStuff/generic-buy-now-pay-later-project-group-56-bnlp/scripts/get_external_data.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(root_dir \u001b[39m+\u001b[39;49m external_data_dir)\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../data/tables/test_external_datasets'"
     ]
    }
   ],
   "source": [
    "# Create \"external_datasets\" folder under the root directory where all the external data will be stored.\n",
    "\n",
    "external_data_dir = 'test_external_datasets'\n",
    "os.makedirs(root_dir + external_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root_dir + external_data_dir + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting external datasets**<br>\n",
    "1) URL retrieve:<br>\n",
    "a) Postcode and SA2 data csv file.<br>\n",
    "b) Total income 2014-2019 excel file.<br>\n",
    "c) Shapefile for states.<br>\n",
    "d) Shapefile for post-codes.<br>\n",
    "2) API call:<br>\n",
    "Population data 2001-2021 csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) Postcode and SA2 data:\n",
    "\n",
    "url = \"https://www.matthewproctor.com/Content/postcodes/australian_postcodes.csv\"\n",
    "r = requests.get(url)\n",
    "target_dir = path + 'postcode_SA2_data.csv'\n",
    "\n",
    "with open(target_dir, 'wb') as outfile:\n",
    "    outfile.write(r.content)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Total income 2014-2019 excel file:\n",
    "\n",
    "url = 'https://www.abs.gov.au/statistics/labour/earnings-and-working-conditions/personal-income-australia/2014-15-2018-19/6524055002_DO001.xlsx'\n",
    "r = requests.get(url)\n",
    "target_dir = path + 'income_data.xlsx'\n",
    "\n",
    "with open(target_dir, 'wb') as outfile:\n",
    "    outfile.write(r.content)\n",
    "    outfile.close()\n",
    "\n",
    "# GO THROUGH WITH NOAH ***********************************************************************\n",
    "# Convert needed sheet from excel file to csv format, and then delete the excel file.\n",
    "# Is this in the correct format you want it Noah for your pre-processing functions to run?\n",
    "read_file = pd.read_excel(target_dir, sheet_name='Table 1.4')\n",
    "os.remove(target_dir)\n",
    "\n",
    "target_dir = path + 'income_data_raw.csv'\n",
    "read_file.to_csv(target_dir, index = None)\n",
    "# df = pd.read_excel('data.xlsx', sheet_name=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c) Australian state shapefiles:\n",
    "\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/STE_2021_AUST_SHP_GDA2020.zip\"\n",
    "target_dir = path + 'state_data.zip'\n",
    "urlretrieve(url, target_dir)\n",
    "\n",
    "# unzip state_data.zip\n",
    "with zipfile.ZipFile(target_dir,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(path + \"state_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1d) Australian post-code shapefiles:\n",
    "\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/POA_2021_AUST_GDA94_SHP.zip\"\n",
    "target_dir = path + 'postcode_data.zip'\n",
    "urlretrieve(url, target_dir)\n",
    "\n",
    "# unzip state_data.zip\n",
    "with zipfile.ZipFile(target_dir,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(path + \"postcode_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) API call:\n",
    "\n",
    "# Set up API connection.\n",
    "\n",
    "WFS_USERNAME = 'xrjps'\n",
    "WFS_PASSWORD= 'Jmf16l4TcswU3Or7'\n",
    "WFS_URL='https://adp.aurin.org.au/geoserver/wfs'\n",
    "\n",
    "adp_client = WebFeatureService(url=WFS_URL,username=WFS_USERNAME, password=WFS_PASSWORD, version='2.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract files and store into external dataset folder directory.\n",
    "\n",
    "response = adp_client.getfeature(typename='datasource-AU_Govt_ABS-UoM_AURIN_DB_3:abs_regional_population_sa2_2001_2021', outputFormat='csv')\n",
    "target_dir = path + 'population_data.csv'\n",
    "\n",
    "out = open(target_dir, 'wb')\n",
    "out.write(response.read())\n",
    "out.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
